{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaeb6e6b-ad22-4ecf-96d5-bf77c0ab8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b57466-1a3d-4625-aac0-c16a53e4035f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Market.01a_OE</th>\n",
       "      <th>Market.01b_OE</th>\n",
       "      <th>Market.01c_OE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Number of people inside the shop, number of pe...</td>\n",
       "      <td>Number of people inside the shop and number of...</td>\n",
       "      <td>We will definitively need to know the amount o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>what causes a door to open automatically.</td>\n",
       "      <td>what can cause a door to open automatically</td>\n",
       "      <td>i think this piece of information is important...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>How many people are going through the doors. H...</td>\n",
       "      <td>How long it will take for everyone to go throu...</td>\n",
       "      <td>We must know this information to decrease the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The groups of customers that might go in, the ...</td>\n",
       "      <td>customers that go in and leave</td>\n",
       "      <td>Because we need to how many customers go in, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>How big is the store, as a team of constructio...</td>\n",
       "      <td>What does the owner want. How tall the store i...</td>\n",
       "      <td>This can impact how the finished product will ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                      Market.01a_OE  \\\n",
       "0      3  Number of people inside the shop, number of pe...   \n",
       "1      0          what causes a door to open automatically.   \n",
       "2      1  How many people are going through the doors. H...   \n",
       "3      1  The groups of customers that might go in, the ...   \n",
       "4      1  How big is the store, as a team of constructio...   \n",
       "\n",
       "                                       Market.01b_OE  \\\n",
       "0  Number of people inside the shop and number of...   \n",
       "1        what can cause a door to open automatically   \n",
       "2  How long it will take for everyone to go throu...   \n",
       "3                     customers that go in and leave   \n",
       "4  What does the owner want. How tall the store i...   \n",
       "\n",
       "                                       Market.01c_OE  \n",
       "0  We will definitively need to know the amount o...  \n",
       "1  i think this piece of information is important...  \n",
       "2  We must know this information to decrease the ...  \n",
       "3  Because we need to how many customers go in, a...  \n",
       "4  This can impact how the finished product will ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"/workspace/output.xlsx\")\n",
    "df = df.iloc[:,1:]\n",
    "df.fillna('', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488d363a-6afb-49cd-b7d3-8f04402e9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concat_response'] = df.iloc[:,1] + ' [SEP] ' + df.iloc[:,2] + ' [SEP] ' + df.iloc[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbdeca1-31cc-4234-8c9a-8adab030c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, DistilBertPreTrainedModel, RobertaForSequenceClassification\n",
    "#from transformers.modeling_distilbert import DistilBertModel, DistilBertPreTrainedModel\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class DistilRobertaClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DistilRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.classifier = torch.nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        roberta_output = outputs[0]\n",
    "\n",
    "        cls_output = roberta_output[:, 0]\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "\n",
    "class RobertaDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.columns = [\"concat_response\"]\n",
    "        self.labels = self.dataframe['Score'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        row = self.dataframe.iloc[index]\n",
    "        response = row['concat_response']\n",
    "        score = row['Score']\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoding_a = self.tokenizer.encode_plus(response, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(encoding_a['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_a': torch.tensor(encoding_a['attention_mask'], dtype=torch.long),\n",
    "            'score': torch.tensor(score, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a16ad61-45cd-401e-b4b4-20835e996ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilRobertaClassifier(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', output_attentions=False)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state = 42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RobertaDataset(train_df, tokenizer)\n",
    "val_dataset = RobertaDataset(val_df, tokenizer)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "model = DistilRobertaClassifier(5)\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0dd2010-041c-497a-8beb-331353bd97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9889290-8d2c-408f-90ce-453f0b43f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.41\n",
      "Average training loss: 0.94\n",
      "Average training loss: 0.72\n",
      "Average training loss: 0.58\n",
      "Average training loss: 0.43\n",
      "Average training loss: 0.30\n",
      "Average training loss: 0.30\n",
      "Average training loss: 0.18\n",
      "Average training loss: 0.20\n",
      "Average training loss: 0.31\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device)\n",
    "        )\n",
    "        loss = loss_function(outputs, batch['score'].to(device).long())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "31c130e5ffad82ae"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b0039cb-45f7-483d-8c2a-dc6c95c2e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 69.17%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device)\n",
    "      )\n",
    "      #all_predictions.extend(outputs.cpu().numpy())\n",
    "      #all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "      _, outputs = torch.max(outputs, 1)\n",
    "      all_predictions.extend(outputs.cpu().numpy())\n",
    "      all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "# Since my predictions return float number, such as 2.3 and 3.5, I decide to round\n",
    "# or map the number in the following way:\n",
    "# 2.5 -> 3; 2.3 -> 2; to get a better algorithm to calculate the accuracy\n",
    "\n",
    "def arrayround(arr,n=0):\n",
    "    import numpy as np\n",
    "    flag = np.where(arr>=0,1,-1)\n",
    "    arr = np.abs(arr)\n",
    "    arr10 = arr*10**(n+1)\n",
    "    arr20 = np.floor(arr10)\n",
    "    arr30 = np.where(arr20%10==5,(arr20+1)/10**(n+1),arr20/10**(n+1))\n",
    "    result = np.around(arr30,n)\n",
    "    return result*flag\n",
    "\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "#all_predictions = arrayround(all_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Compute the average accuracy over all batches\n",
    "correct_predictions = sum(all_predictions == np.array(all_labels))\n",
    "total_predictions = len(all_predictions)\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d847e1b4-e110-4b6e-8ac2-4b15eb1ebb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
