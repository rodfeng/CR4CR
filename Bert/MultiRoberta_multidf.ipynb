{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e117a5-51af-4f23-b852-45cd8a949db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:37.349518300Z",
     "start_time": "2024-02-19T05:51:37.344012700Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_location = \"/Users/Xutao/Documents/CR4CR/data/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:37.685408100Z",
     "start_time": "2024-02-19T05:51:37.679901500Z"
    }
   },
   "id": "74bd3f678d8398a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3e3019-b787-42bc-82be-67abda637a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:37.893216100Z",
     "start_time": "2024-02-19T05:51:37.864952300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Score                                      Market.00a_OE  \\\n0      3  Number of people inside the shop, number of pe...   \n1      0          what causes a door to open automatically.   \n2      1  How many people are going through the doors. H...   \n3      1  The groups of customers that might go in, the ...   \n4      1  How big is the store, as a team of constructio...   \n\n                                       Market.00b_OE  \\\n0  Number of people inside the shop and number of...   \n1        what can cause a door to open automatically   \n2  How long it will take for everyone to go throu...   \n3                     customers that go in and leave   \n4  What does the owner want. How tall the store i...   \n\n                                       Market.00c_OE  \n0  We will definitively need to know the amount o...  \n1  i think this piece of information is important...  \n2  We must know this information to decrease the ...  \n3  Because we need to how many customers go in, a...  \n4  This can impact how the finished product will ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n      <th>Market.00a_OE</th>\n      <th>Market.00b_OE</th>\n      <th>Market.00c_OE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Number of people inside the shop, number of pe...</td>\n      <td>Number of people inside the shop and number of...</td>\n      <td>We will definitively need to know the amount o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>what causes a door to open automatically.</td>\n      <td>what can cause a door to open automatically</td>\n      <td>i think this piece of information is important...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>How many people are going through the doors. H...</td>\n      <td>How long it will take for everyone to go throu...</td>\n      <td>We must know this information to decrease the ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>The groups of customers that might go in, the ...</td>\n      <td>customers that go in and leave</td>\n      <td>Because we need to how many customers go in, a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>How big is the store, as a team of constructio...</td>\n      <td>What does the owner want. How tall the store i...</td>\n      <td>This can impact how the finished product will ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(dataset_location + \"output.xlsx\")\n",
    "df = df.iloc[:,1:]\n",
    "df.fillna('', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8ea2d7-fb65-4da5-8876-f5dcb2ac7766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:38.119787800Z",
     "start_time": "2024-02-19T05:51:38.113135700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertPreTrainedModel, RobertaForSequenceClassification\n",
    "#from transformers.modeling_distilbert import DistilBertModel, DistilBertPreTrainedModel\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class MultimodalRoberta(torch.nn.Module):\n",
    "    def __init__(self, num_labels=5):\n",
    "        super(MultimodalRoberta, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta1 = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.roberta2 = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.roberta3 = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.classifier = nn.Linear(self.roberta1.config.hidden_size + self.roberta2.config.hidden_size + self.roberta3.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b, input_ids_c, attention_mask_c):\n",
    "        output_a = self.roberta1(input_ids=input_ids_a, attention_mask=attention_mask_a)\n",
    "        output_b = self.roberta2(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "        output_c = self.roberta3(input_ids=input_ids_c, attention_mask=attention_mask_c)\n",
    "\n",
    "        concatenated_output = torch.cat((output_a.pooler_output, output_b.pooler_output, output_c.pooler_output), 1)\n",
    "\n",
    "        return self.classifier(concatenated_output)\n",
    "\n",
    "\n",
    "class MultimodalRobertaDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.columns = [\"Market.00a_OE\", \"Market.00b_OE\", \"Market.00c_OE\"]\n",
    "        self.labels = self.dataframe['Score'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        row = self.dataframe.iloc[index]\n",
    "        response_a = row['Market.00a_OE']\n",
    "        response_b = row['Market.00b_OE']\n",
    "        response_c = row['Market.00c_OE']\n",
    "        score = row['Score']\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoding_a = self.tokenizer.encode_plus(response_a, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "        encoding_b = self.tokenizer.encode_plus(response_b, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "        encoding_c = self.tokenizer.encode_plus(response_c, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(encoding_a['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_a': torch.tensor(encoding_a['attention_mask'], dtype=torch.long),\n",
    "            'input_ids_b': torch.tensor(encoding_b['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_b': torch.tensor(encoding_b['attention_mask'], dtype=torch.long),\n",
    "            'input_ids_c': torch.tensor(encoding_c['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_c': torch.tensor(encoding_c['attention_mask'], dtype=torch.long),\n",
    "            'score': torch.tensor(score, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2560c9-d250-4544-acb8-939b51a7a54d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:39.991432700Z",
     "start_time": "2024-02-19T05:51:38.545646400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MultimodalRoberta(\n  (roberta1): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (roberta2): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (roberta3): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): Linear(in_features=2304, out_features=5, bias=True)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', output_attentions=False)\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state = 42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalRobertaDataset(train_df, tokenizer)\n",
    "val_dataset = MultimodalRobertaDataset(val_df, tokenizer)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "model = MultimodalRoberta()\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:39.992432300Z",
     "start_time": "2024-02-19T05:51:39.989063400Z"
    }
   },
   "id": "d9a183f588be117e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2858e9c4-a7ca-43a6-a49b-675b58fa2e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:40.181564600Z",
     "start_time": "2024-02-19T05:51:40.177754900Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97aa290c-8d13-4b07-a9f5-baaef1eec8f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:54:09.076564700Z",
     "start_time": "2024-02-19T05:51:41.266855200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.23\n",
      "Average training loss: 0.72\n",
      "Average training loss: 0.44\n",
      "Average training loss: 0.30\n",
      "Average training loss: 0.19\n",
      "Average training loss: 0.09\n",
      "Average training loss: 0.05\n",
      "Average training loss: 0.02\n",
      "Average training loss: 0.01\n",
      "Average training loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "            batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "            batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "        )\n",
    "        loss = loss_function(outputs, batch['score'].to(device).long())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd46fde-5c67-4135-abd4-a8361531f22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:17.113103300Z",
     "start_time": "2024-02-19T05:51:17.110076800Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "          batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "          batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "      )\n",
    "      #all_predictions.extend(outputs.cpu().numpy())\n",
    "      #all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "      _, outputs = torch.max(outputs, 1)\n",
    "      all_predictions.extend(outputs.cpu().numpy())\n",
    "      all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "# Since my predictions return float number, such as 2.3 and 3.5, I decide to round\n",
    "# or map the number in the following way:\n",
    "# 2.5 -> 3; 2.3 -> 2; to get a better algorithm to calculate the accuracy\n",
    "\n",
    "def arrayround(arr,n=0):\n",
    "    import numpy as np\n",
    "    flag = np.where(arr>=0,1,-1)\n",
    "    arr = np.abs(arr)\n",
    "    arr10 = arr*10**(n+1)\n",
    "    arr20 = np.floor(arr10)\n",
    "    arr30 = np.where(arr20%10==5,(arr20+1)/10**(n+1),arr20/10**(n+1))\n",
    "    result = np.around(arr30,n)\n",
    "    return result*flag\n",
    "\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "#all_predictions = arrayround(all_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Compute the average accuracy over all batches\n",
    "correct_predictions = sum(all_predictions == np.array(all_labels))\n",
    "total_predictions = len(all_predictions)\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load a new dataset for prediction\n",
    "import re\n",
    "prediction_data = pd.read_excel(dataset_location + \"Fall 23 Market_roderick.xlsx\")\n",
    "prediction_data = prediction_data.rename(columns={\"Market.00bc_OE\":\"Market.00b_OE\", \"Market.00bc_OE follow up\": 'Market.00c_OE', \"Waypoints\": \"Score\"})\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    # text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    #text = re.sub(r'^\\w+\\s*$', '',text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Ensure data types of responses are string\n",
    "for col in [\"Market.00a_OE\", \"Market.00b_OE\", \"Market.00c_OE\"]:\n",
    "  prediction_data[col] = prediction_data[col].astype(str).apply(preprocess_text)\n",
    "  \n",
    "prediction_dataset = MultimodalRobertaDataset(prediction_data, tokenizer)\n",
    "prediction_dataloader = DataLoader(prediction_dataset, batch_size=2)\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in prediction_dataloader:\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "          batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "          batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "      )\n",
    "      #all_predictions.extend(outputs.cpu().numpy())\n",
    "      #all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "      _, outputs = torch.max(outputs, 1)\n",
    "      all_predictions.extend(outputs.cpu().numpy())\n",
    "      all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "\n",
    "# add one more column to the dataset\n",
    "prediction_data['predict_by_model'] = all_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.112147500Z"
    }
   },
   "id": "a4d715148cf8203e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prediction_data.to_excel(dataset_location + \"Fall 2023 Market_roderick and model comparison.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:17.113610500Z",
     "start_time": "2024-02-19T05:51:17.113103300Z"
    }
   },
   "id": "130d084eac1974a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d2be0-8147-4a3e-8070-0350694e2337",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.114116900Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the hyperparameter search space\n",
    "learning_rates = [1e-5]  # List of learning rates to try\n",
    "batch_sizes = [4, 8, 16, 32]  # List of batch sizes to try\n",
    "epoch_sizes = [5,6,7,8,9,10]\n",
    "combination_accuracies = {}\n",
    "\n",
    "for epoch_size in epoch_sizes:\n",
    "  for batch_size in batch_sizes:\n",
    "    # Re-training the model for each combination of hyperparameters\n",
    "    model = MultimodalRoberta()\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model.train()\n",
    "    print([epoch_size, batch_size]) # prints out the hyperparameter combination being tested\n",
    "    for epoch in range(epoch_size):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "                batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "                batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "            )\n",
    "            loss = loss_function(outputs, batch['score'].to(device).long())\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to keep track of predictions and ground truth labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Evaluate on the test dataset\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "          # Forward pass\n",
    "          outputs = model(\n",
    "              batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "              batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "              batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "          )\n",
    "          #all_predictions.extend(outputs.cpu().numpy())\n",
    "          #all_labels.extend(batch['score'].numpy())\n",
    "    \n",
    "          _, outputs = torch.max(outputs, 1)\n",
    "          all_predictions.extend(outputs.cpu().numpy())\n",
    "          all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    all_predictions = np.array(all_predictions).flatten()\n",
    "\n",
    "    correct_predictions = sum(all_predictions == np.array(all_labels))\n",
    "    total_predictions = len(all_predictions)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "\n",
    "    combination_accuracies[(epoch_size, batch_size)] = test_accuracy\n",
    "\n",
    "    # Deletes the cache and the model from GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff443da-309c-4cd9-9fc0-52025874491f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.115123600Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5dbf4-2adf-4765-a24d-93185d93a81f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.116123800Z"
    }
   },
   "outputs": [],
   "source": [
    "val_df[['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a8325-debf-429b-8cdd-9a76d6aefd45",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.117123900Z"
    }
   },
   "outputs": [],
   "source": [
    "val_df['predicted_score'] = all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204f385-f624-4458-931d-0d62d19d43b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:17.119122300Z",
     "start_time": "2024-02-19T05:51:17.118121800Z"
    }
   },
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644958c9-e50d-4e08-8488-5426f706b269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:51:17.119122300Z",
     "start_time": "2024-02-19T05:51:17.119122300Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df.to_excel(\"/workspace/difference.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c2510-5cf6-4695-b74d-768ac49c0375",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.120123Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = val_df[val_df[\"Score\"] != val_df[\"predicted_score\"]]\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ff27a-428a-4584-b168-6545e4e57089",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-19T05:51:17.121122100Z"
    }
   },
   "outputs": [],
   "source": [
    "len(val_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
