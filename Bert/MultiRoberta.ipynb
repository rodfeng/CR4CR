{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f7ed9e-4ea0-4b6c-855d-6bded6d81198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44823c7d-9318-47b0-920d-94c34b461adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/workspace/DCS Market.01abc_OEOEOE Combined.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6503c7d4-6795-4eee-b1e3-49b5beecd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data cleaning to ensure score is not \"missing\" or NaN, etc\n",
    "\n",
    "df = df[df['Score'].isin(range(5))]\n",
    "\n",
    "columns = [\"Market.01a_OE\", \"Market.01b_OE\", \"Market.01c_OE\"]\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\+\\-\\s]', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # text = re.sub(r'^\\w+\\s*$', '',text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Ensure data types of responses are string\n",
    "for col in columns:\n",
    "  df[col] = df[col].astype(str).apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61603c6-c3b5-40ba-942a-b42a4b7395b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, DistilBertPreTrainedModel, RobertaForSequenceClassification\n",
    "#from transformers.modeling_distilbert import DistilBertModel, DistilBertPreTrainedModel\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class MultimodalRoberta(torch.nn.Module):\n",
    "    def __init__(self, num_labels=5):\n",
    "        super(MultimodalRoberta, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta1 = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.roberta2 = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.roberta3 = RobertaModel.from_pretrained('distilroberta-base')\n",
    "        self.classifier = nn.Linear(self.roberta1.config.hidden_size + self.roberta2.config.hidden_size + self.roberta3.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b, input_ids_c, attention_mask_c):\n",
    "        output_a = self.roberta1(input_ids=input_ids_a, attention_mask=attention_mask_a)\n",
    "        output_b = self.roberta2(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "        output_c = self.roberta3(input_ids=input_ids_c, attention_mask=attention_mask_c)\n",
    "\n",
    "        concatenated_output = torch.cat((output_a.pooler_output, output_b.pooler_output, output_c.pooler_output), 1)\n",
    "\n",
    "        return self.classifier(concatenated_output)\n",
    "\n",
    "\n",
    "class MultimodalRobertaDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.columns = [\"Market.01a_OE\", \"Market.01b_OE\", \"Market.01c_OE\"]\n",
    "        self.labels = self.dataframe['Score'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        row = self.dataframe.iloc[index]\n",
    "        response_a = row['Market.01a_OE']\n",
    "        response_b = row['Market.01b_OE']\n",
    "        response_c = row['Market.01c_OE']\n",
    "        score = row['Score']\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoding_a = self.tokenizer.encode_plus(response_a, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "        encoding_b = self.tokenizer.encode_plus(response_b, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "        encoding_c = self.tokenizer.encode_plus(response_c, add_special_tokens=True, max_length=self.max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(encoding_a['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_a': torch.tensor(encoding_a['attention_mask'], dtype=torch.long),\n",
    "            'input_ids_b': torch.tensor(encoding_b['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_b': torch.tensor(encoding_b['attention_mask'], dtype=torch.long),\n",
    "            'input_ids_c': torch.tensor(encoding_c['input_ids'], dtype=torch.long),\n",
    "            'attention_mask_c': torch.tensor(encoding_c['attention_mask'], dtype=torch.long),\n",
    "            'score': torch.tensor(score, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30724f31-370b-417c-89e7-2151fcf3471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalRoberta(\n",
       "  (roberta1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (roberta2): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (roberta3): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', output_attentions=False)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state = 42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalRobertaDataset(train_df, tokenizer)\n",
    "val_dataset = MultimodalRobertaDataset(val_df, tokenizer)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "model = MultimodalRoberta()\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89fbbdd1-4cfc-4b2a-9d8b-ec7d9ae411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ec32b9-d149-40cc-a0e9-2b8916ae20b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.59\n",
      "Average training loss: 1.55\n",
      "Average training loss: 1.52\n",
      "Average training loss: 1.44\n",
      "Average training loss: 1.28\n",
      "Average training loss: 1.07\n",
      "Average training loss: 0.87\n",
      "Average training loss: 0.73\n",
      "Average training loss: 0.60\n",
      "Average training loss: 0.50\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "            batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "            batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "        )\n",
    "        loss = loss_function(outputs, batch['score'].to(device).long())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3491f3ee-ecde-4549-89c7-d8114eadefaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 70.59%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "          batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "          batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "      )\n",
    "      #all_predictions.extend(outputs.cpu().numpy())\n",
    "      #all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "      _, outputs = torch.max(outputs, 1)\n",
    "      all_predictions.extend(outputs.cpu().numpy())\n",
    "      all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "# Since my predictions return float number, such as 2.3 and 3.5, I decide to round\n",
    "# or map the number in the following way:\n",
    "# 2.5 -> 3; 2.3 -> 2; to get a better algorithm to calculate the accuracy\n",
    "\n",
    "def arrayround(arr,n=0):\n",
    "    import numpy as np\n",
    "    flag = np.where(arr>=0,1,-1)\n",
    "    arr = np.abs(arr)\n",
    "    arr10 = arr*10**(n+1)\n",
    "    arr20 = np.floor(arr10)\n",
    "    arr30 = np.where(arr20%10==5,(arr20+1)/10**(n+1),arr20/10**(n+1))\n",
    "    result = np.around(arr30,n)\n",
    "    return result*flag\n",
    "\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "#all_predictions = arrayround(all_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Compute the average accuracy over all batches\n",
    "correct_predictions = sum(all_predictions == np.array(all_labels))\n",
    "total_predictions = len(all_predictions)\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14fc14bb-59e7-472d-b673-bdad8c1bf79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4]\n",
      "Test Accuracy: 56.86%\n",
      "[4, 8]\n",
      "Test Accuracy: 62.75%\n",
      "[4, 16]\n",
      "Test Accuracy: 62.75%\n",
      "[4, 32]\n",
      "Test Accuracy: 47.06%\n",
      "[5, 4]\n",
      "Test Accuracy: 68.63%\n",
      "[5, 8]\n",
      "Test Accuracy: 62.75%\n",
      "[5, 16]\n",
      "Test Accuracy: 60.78%\n",
      "[5, 32]\n",
      "Test Accuracy: 62.75%\n",
      "[6, 4]\n",
      "Test Accuracy: 64.71%\n",
      "[6, 8]\n",
      "Test Accuracy: 68.63%\n",
      "[6, 16]\n",
      "Test Accuracy: 68.63%\n",
      "[6, 32]\n",
      "Test Accuracy: 58.82%\n",
      "[7, 4]\n",
      "Test Accuracy: 60.78%\n",
      "[7, 8]\n",
      "Test Accuracy: 60.78%\n",
      "[7, 16]\n",
      "Test Accuracy: 66.67%\n",
      "[7, 32]\n",
      "Test Accuracy: 62.75%\n",
      "[8, 4]\n",
      "Test Accuracy: 52.94%\n",
      "[8, 8]\n",
      "Test Accuracy: 58.82%\n",
      "[8, 16]\n",
      "Test Accuracy: 64.71%\n",
      "[8, 32]\n",
      "Test Accuracy: 64.71%\n",
      "[9, 4]\n",
      "Test Accuracy: 70.59%\n",
      "[9, 8]\n",
      "Test Accuracy: 70.59%\n",
      "[9, 16]\n",
      "Test Accuracy: 70.59%\n",
      "[9, 32]\n",
      "Test Accuracy: 66.67%\n",
      "[10, 4]\n",
      "Test Accuracy: 62.75%\n",
      "[10, 8]\n",
      "Test Accuracy: 60.78%\n",
      "[10, 16]\n",
      "Test Accuracy: 60.78%\n",
      "[10, 32]\n",
      "Test Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the hyperparameter search space\n",
    "learning_rates = [1e-5]  # List of learning rates to try\n",
    "batch_sizes = [4, 8, 16, 32]  # List of batch sizes to try\n",
    "epoch_sizes = [4,5,6,7,8,9,10]\n",
    "combination_accuracies = {}\n",
    "\n",
    "for epoch_size in epoch_sizes:\n",
    "  for batch_size in batch_sizes:\n",
    "    # Re-training the model for each combination of hyperparameters\n",
    "    model = MultimodalRoberta()\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model.train()\n",
    "    print([epoch_size, batch_size]) # prints out the hyperparameter combination being tested\n",
    "    for epoch in range(epoch_size):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "                batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "                batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "            )\n",
    "            loss = loss_function(outputs, batch['score'].to(device).long())\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to keep track of predictions and ground truth labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Evaluate on the test dataset\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "          # Forward pass\n",
    "          outputs = model(\n",
    "              batch['input_ids_a'].to(device), batch['attention_mask_a'].to(device),\n",
    "              batch['input_ids_b'].to(device), batch['attention_mask_b'].to(device),\n",
    "              batch['input_ids_c'].to(device), batch['attention_mask_c'].to(device)\n",
    "          )\n",
    "          #all_predictions.extend(outputs.cpu().numpy())\n",
    "          #all_labels.extend(batch['score'].numpy())\n",
    "    \n",
    "          _, outputs = torch.max(outputs, 1)\n",
    "          all_predictions.extend(outputs.cpu().numpy())\n",
    "          all_labels.extend(batch['score'].numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    all_predictions = np.array(all_predictions).flatten()\n",
    "\n",
    "    correct_predictions = sum(all_predictions == np.array(all_labels))\n",
    "    total_predictions = len(all_predictions)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "\n",
    "    combination_accuracies[(epoch_size, batch_size)] = test_accuracy\n",
    "\n",
    "    # Deletes the cache and the model from GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a7ab1d9-bd46-4a01-88c8-2488187023a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
